{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm\n",
    "from useful_functions import state_evol, random_state, fidelity\n",
    "from data import generates_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first step in our project should be specify the physical system that we are going to work on. For this, we require define the Hamiltonian (i.e. the physical system) and,\n",
    "# for that, we require the Pauli basis.\n",
    "\n",
    "sigma_x = np.array([[0, 1], \n",
    "                    [1, 0]], dtype=complex)\n",
    "\n",
    "sigma_y = np.array([[0, -1j], \n",
    "                    [1j, 0]], dtype=complex)\n",
    "\n",
    "sigma_z = np.array([[1, 0], \n",
    "                    [0, -1]], dtype=complex)\n",
    "\n",
    "pauli_basis = [np.eye(2, dtype=complex), sigma_x, sigma_y, sigma_z]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose a simply Hamiltonian for this project, but feel free to use anyone. Just remember, for a different Hamiltonian is necesarry to train again the model.\n",
    "\n",
    "\\begin{equation}\n",
    "    H = \\frac{1}{\\hbar} \\left( \\sigma_x \\otimes \\sigma_x + \\sigma_z \\otimes \\sigma_z \\right).\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = np.kron(pauli_basis[1], pauli_basis[1]) + np.kron(pauli_basis[3], pauli_basis[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this part we define the architecture for our model. We require two channel inputs for convolutional layers and, in the latent space, we introduce a scalar parameter.\n",
    "# The output is a two channel image that represents the output state, real and imaginary part.\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=2, out_channels=8, kernel_size=2)  # Output: 8 x 3 x 3\n",
    "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=2)  # Output: 16 x 2 x 2\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(16 * 2 * 2 + 1, 32)  # Flattened vector + scalar parameter\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.fc3 = nn.Linear(32, 4 * 4 * 2)  # Output for the 4x4x2 image\n",
    "        \n",
    "    def forward(self, x, scalar_param):\n",
    "        # Convolutional layers with Tanh activation\n",
    "        x = torch.tanh(self.conv1(x))  # Output: 8 x 3 x 3\n",
    "        x = torch.tanh(self.conv2(x))  # Output: 16 x 2 x 2\n",
    "        \n",
    "        # Flatten the output of the convolutional layer\n",
    "        x = x.flatten()  # Flatten: 16*2*2 = 64\n",
    "        x = torch.cat((x, scalar_param))  # Concatenate: 64 + 1 = 65\n",
    "        \n",
    "        # Fully connected layers with Tanh activation\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        \n",
    "        # Output layer\n",
    "        x = torch.tanh(self.fc3(x))\n",
    "        \n",
    "        # Reshape to obtain the output with size 4x4x2\n",
    "        x = x.view(2, 4, 4)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For train our model an optimizer and matrics must be specified\n",
    "\n",
    "def metric(predicted, target):\n",
    "    rho_model   = predicted[0] + 1j*predicted[1]\n",
    "    rho_target  = target[0] + 1j*target[1]\n",
    "    rho_target  = rho_target\n",
    "\n",
    "    U, S, Vh    = torch.linalg.svd(rho_model)\n",
    "    SP          = U @ torch.diag(torch.complex(S, torch.zeros_like(S))) @ torch.conj(U).T\n",
    "    SP          = SP/torch.trace(SP)\n",
    "    \n",
    "    return 1 - fidelity(SP, rho_target)\n",
    "\n",
    "optimizer   = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the problems of Machine Learning with Quantum Information is that the outcomes states aren't perfectly satisfies the requirements for to be quantum states, that is, trace one and semi-positivity. This is because using hard-constraint to impose those conditions not secure to be satisfies at 100%.\n",
    "\n",
    "A solution for that is consider a polar descomposition for the Neural Network output and, then, apply a soft-contrain for those requirements, such as renormalization for impose unitary trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters for training, that is, \n",
    "#   N_train for the number of iterations for train our model\n",
    "#   N_data for the number of generated data for each train interation\n",
    "#   We also define a loss_train list for append the loss function values for analyze the convergence of our model\n",
    "\n",
    "N_train = 250\n",
    "N_data  = 1000\n",
    "\n",
    "loss_train = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that n_qubits = 2 for the Hamiltonian system related to a two-qubit state\n",
    "\n",
    "for train in tqdm(range(N_train), desc=\"Training Epochs\"):\n",
    "\n",
    "    # Generate the training data for this epoch\n",
    "    input_data, output_data = generates_data(N_data=N_data, n_qubits=2, Hamiltonian=H)\n",
    "    \n",
    "    # Initialize loss for this epoch\n",
    "    train_loss = 0\n",
    "\n",
    "    # Loop through each data point in the generated dataset\n",
    "    for n in range(N_data):\n",
    "        model.train()  # Set the model to training mode\n",
    "        optimizer.zero_grad()  # Reset gradients for this batch\n",
    "\n",
    "        # Forward pass: predict the output using the model\n",
    "        prediction = model(input_data[0][n], input_data[1][n])  # input_data[0] are states, input_data[1] are times\n",
    "        targets = output_data[n]  # Target values are the final states\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = metric(prediction, targets)\n",
    "\n",
    "        # Backpropagation: compute gradients and update the model parameters\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate the training loss for this epoch\n",
    "        train_loss += loss.item() / N_data\n",
    "\n",
    "    # Store the average loss for this epoch\n",
    "    loss_train.append(train_loss)\n",
    "\n",
    "    # Optionally, print or log training loss for this epoch\n",
    "    # print(f\"Epoch {train+1}/{N_train}, Loss: {train_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
